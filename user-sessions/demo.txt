
This research is built around better understanding linguistic relationships to attention mechanisms in cross-lingual transfer. It stands to offer important insights on multilingual relationships. This may include pre-existing relationships from the multilingual pretrained models which could highlight how languages are already clustered. By focusing on languages with morphological or typological similarities, we're able to yield important results about which language relationships draw more direct parallels. We empirically investigate series of attention maps while also testing the viability of both zero-shot and few-shot transfer to highlight whether mBERT as it stands is an effective model choice. This stands to significantly help shed light on the low resource language problem aiding communities that are underrepresented in NLP. 
Use: 0 

This research is built around better understanding linguistic relationships to attention mechanisms in cross-lingual transfer. It stands to offer important insights on multilingual relationships. This may include pre-existing relationships from the multilingual pretrained models which could highlight how languages are already clustered. By focusing on languages with morphological or typological similarities, we're able to yield important results about which language relationships draw more direct parallels. We empirically investigate series of attention maps while also testing the viability of both zero-shot and few-shot transfer to highlight whether mBERT as it stands is an effective model choice. This stands to significantly help shed light on the low resource language problem aiding communities that are underrepresented in NLP. Unfortunately, in this same vein, misrepresentations could lead to continuously overlooking languages or poor translations which create language barriers that will harm the low-resource communities. 
Use: 0 
LABEL WRONG 
 

This research is built around better understanding linguistic relationships to attention mechanisms in cross-lingual transfer. It stands to offer important insights on multilingual relationships. This may include pre-existing relationships from the multilingual pretrained models which could highlight how languages are already clustered. By focusing on languages with morphological or typological similarities, we're able to yield important results about which language relationships draw more direct parallels. We empirically investigate series of attention maps while also testing the viability of both zero-shot and few-shot transfer to highlight whether mBERT as it stands is an effective model choice. This stands to significantly help shed light on the low resource language problem aiding communities that are underrepresented in NLP. Unfortunately, in this same vein, misrepresentations could lead to continuously overlooking languages or poor translations. Our models could harm low-resource communities by enforcing language barriers. 
Use: 0 
LABEL WRONG 
 

This research is built around better understanding linguistic relationships to attention mechanisms in cross-lingual transfer. It stands to offer important insights on multilingual relationships. This may include pre-existing relationships from the multilingual pretrained models which could highlight how languages are already clustered. By focusing on languages with morphological or typological similarities, we're able to yield important results about which language relationships draw more direct parallels. This stands to significantly help shed light on the low resource language problem aiding communities that are underrepresented in NLP. However, misrepresentations will also directly negatively harm low resource communities by creating new language barriers. 
Use: 0 
